<!DOCTYPE html>
<html lang="uz">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Mento AI â€” Inclusive Learning Demo</title>
  <script src="https://cdn.tailwindcss.com"></script>
  <!-- MediaPipe Hands (optional) -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands/hands.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
</head>
<body class="bg-gray-50 text-gray-900">

  <!-- HERO -->
  <header class="min-h-screen flex flex-col justify-center items-center text-center p-8 bg-gradient-to-b from-indigo-700 to-indigo-500 text-white">
    <h1 class="text-5xl md:text-6xl font-extrabold mb-4">Mento AI â€” Inclusive Learning</h1>
    <p class="text-lg md:text-2xl max-w-3xl">
      Ta'limda hamma uchun imkoniyat: ovozdan matnga, matndan ovozga va qoâ€˜l harakati bilan boshqariladigan interaktiv doska.
    </p>
    <div class="mt-8 flex gap-4">
      <a href="#voice-to-text" class="bg-white/20 px-6 py-3 rounded-lg hover:bg-white/30">Boshlash</a>
      <a href="#demo" class="bg-white text-indigo-700 px-6 py-3 rounded-lg font-semibold">Demo sahifa</a>
    </div>
  </header>

  <!-- PROBLEM/SOLUTION (short) -->
  <section id="problem" class="py-12 px-6 max-w-5xl mx-auto">
    <h2 class="text-3xl font-bold mb-4 text-center">Muammo â†’ Yechim</h2>
    <p class="text-center text-gray-700">
      Ta'limda eshitish yoki koâ€˜rish bilan muammosi boâ€˜lgan oâ€˜quvchilar uchun darsni tushunish qiyin boâ€˜lishi mumkin.
      Mento AI â€” Voiceâ†’Text, Textâ†’Voice va Gesture Recognition orqali inklyuziv oâ€˜qitishni qoâ€˜llab-quvvatlaydi.
    </p>
  </section>

  <!-- VOICE TO TEXT -->
  <section id="voice-to-text" class="py-12 px-6 bg-white">
    <div class="max-w-4xl mx-auto">
      <h2 class="text-2xl font-bold mb-4">1ï¸âƒ£ Voice â†’ Text (Ovozdan matnga)</h2>
      <p class="text-gray-700 mb-4">
        Ushbu boâ€˜lim oâ€˜qituvchining nutqini real-time matnga aylantiradi â€” eshitishda qiyinchilik koâ€˜radigan bolalar uchun subtitr.
        Demo: brauzerning SpeechRecognition (agar qoâ€˜llab-quvvatlasa) dan foydalanadi. Productionda Whisper/Google STT yoki AssemblyAI ishlatiladi.
      </p>

      <div class="p-4 bg-gray-50 rounded shadow">
        <p class="mb-2 font-semibold">Demo: Brauzer STT (klik qilib gapiring)</p>
        <div class="flex gap-2">
          <button id="startStt" class="px-4 py-2 bg-green-600 text-white rounded">Start (gapirish)</button>
          <button id="stopStt" class="px-4 py-2 bg-red-600 text-white rounded">Stop</button>
        </div>
        <div id="sttText" class="mt-4 p-3 bg-white border rounded min-h-[80px]"></div>
        <p class="mt-2 text-sm text-gray-500">
          Production: Whisper / Google STT uchun audio blobni backendga yuboring (server key required).
        </p>
      </div>

      <h3 class="mt-6 font-semibold">Tavsiyalar (Production)</h3>
      <ul class="list-disc ml-6 text-gray-700">
        <li>OpenAI Whisper (batch yoki streaming) â€” aniq transkript</li>
        <li>Google Speech-to-Text â€” real-time subtitrlar</li>
        <li>AssemblyAI â€” audio segment va speaker diarization</li>
      </ul>
    </div>
  </section>

  <!-- TEXT TO VOICE -->
  <section id="text-to-voice" class="py-12 px-6 bg-gray-100">
    <div class="max-w-4xl mx-auto">
      <h2 class="text-2xl font-bold mb-4">2ï¸âƒ£ Text â†’ Voice (Matndan ovozga)</h2>
      <p class="text-gray-700 mb-4">
        Matnni tabiiy ovozga aylantirish â€” koâ€˜rish qiyinligi boâ€˜lgan oâ€˜quvchilar uchun. Demo brauzer speechSynthesis bilan ishlaydi.
        Productionda ElevenLabs / Amazon Polly / Google TTS yoki Azure ishlatiladi.
      </p>

      <div class="p-4 bg-white rounded shadow">
        <textarea id="ttsText" class="w-full p-2 border rounded" rows="4" placeholder="Matnni shu yerga yozing...">Assalomu alaykum, Mento AI demo â€” bu matndan ovozga misol.</textarea>
        <div class="mt-3 flex gap-2">
          <button onclick="speakText()" class="px-4 py-2 bg-indigo-600 text-white rounded">Oâ€˜qib berish (brauzer)</button>
          <button onclick="stopSpeak()" class="px-4 py-2 bg-gray-400 text-black rounded">Toâ€˜xtatish</button>
        </div>
        <p class="mt-2 text-sm text-gray-500">
          Production: ElevenLabs API bilan oâ€˜qituvchining ovozini klonlash (server orqali).
        </p>
      </div>
    </div>
  </section>

  <!-- GESTURE RECOGNITION -->
  <section id="gesture" class="py-12 px-6 bg-white">
    <div class="max-w-4xl mx-auto">
      <h2 class="text-2xl font-bold mb-4">3ï¸âƒ£ Hand Gesture Recognition (Qoâ€˜l harakati tanish)</h2>
      <p class="text-gray-700 mb-4">
        Qoâ€˜l koâ€˜targan oâ€˜quvchini aniqlash yoki gesture orqali doska boshqaruvi. Demo uchun MediaPipe Hands ishlatiladi.
      </p>

      <div class="grid md:grid-cols-2 gap-4">
        <div class="p-4 bg-gray-50 rounded shadow">
          <video id="gestureVideo" class="w-full rounded" autoplay playsinline muted></video>
          <canvas id="gestureCanvas" class="w-full mt-2 rounded border"></canvas>
        </div>

        <div class="p-4 bg-white rounded shadow">
          <p class="font-semibold">Qanday ishlaydi (demo):</p>
          <ol class="list-decimal ml-6 text-gray-700 mt-2">
            <li>Kamera ruxsatini bering.</li>
            <li>MediaPipe hands model kamera framelarini tahlil qiladi.</li>
            <li>Agar qoâ€˜l koâ€˜tarlgan boâ€˜lsa â€” UI da indikator chiqariladi.</li>
          </ol>
          <p id="gestureStatus" class="mt-4 text-green-600 font-semibold">Status: tayyor</p>
        </div>
      </div>

      <p class="mt-4 text-sm text-gray-500">
        Production: OpenCV + TensorFlow (yoki MediaPipe) bilan edge-device (Raspberry Pi / Jetson) deploy tavsiya etiladi.
      </p>
    </div>
  </section>

  <!-- SMART BOARD INTEGRATION -->
  <section id="smart-board" class="py-12 px-6 bg-gray-100">
    <div class="max-w-5xl mx-auto">
      <h2 class="text-2xl font-bold mb-4">4ï¸âƒ£ Smart Board Integration (Interaktiv doska)</h2>
      <p class="text-gray-700 mb-4">
        Barcha tizimlar (STT, TTS, Gesture) bitta interfeysda birlashadi â€” oâ€˜qituvchi yoki admin real-time statistik va vizualizatsiyani koâ€˜radi.
      </p>

      <div class="bg-white p-4 rounded shadow">
        <p class="font-semibold">Demo dashboard (mock):</p>
        <div class="grid md:grid-cols-3 gap-4 mt-4">
          <div class="p-3 border rounded">
            <h4 class="font-semibold">Live attention</h4>
            <div class="text-3xl mt-2">75%</div>
            <p class="text-sm text-gray-500">Oâ€˜quvchilar e'tibor indeksi</p>
          </div>
          <div class="p-3 border rounded">
            <h4 class="font-semibold">Raised hands</h4>
            <div class="text-3xl mt-2">3</div>
            <p class="text-sm text-gray-500">Hozir qoâ€˜l koâ€˜tarilgan oâ€˜quvchilar</p>
          </div>
          <div class="p-3 border rounded">
            <h4 class="font-semibold">Live transcript</h4>
            <div id="liveTranscript" class="text-sm mt-2 text-gray-700">â€”</div>
            <p class="text-sm text-gray-500">Oâ€˜qituvchi nutqi transkripti</p>
          </div>
        </div>

        <p class="mt-4 text-sm text-gray-600">
          Texnologiyalar: React + Node.js (dashboard), WebSocket (socket.io) orqali real-time data, Python/Edge service kameradan event yuboradi.
        </p>
      </div>
    </div>
  </section>

  <!-- INCLUSIVE LEARNING EXPLANATION -->
  <section id="inclusive" class="py-12 px-6 bg-white">
    <div class="max-w-4xl mx-auto">
      <h2 class="text-2xl font-bold mb-4">ğŸ¯ Inclusive Learning (Inklyuziv ta'lim) haqida</h2>
      <p class="text-gray-700 mb-4">
        Inklyuziv ta'lim â€” bu har bir oâ€˜quvchining oâ€˜ziga xos imkoniyatlarini hisobga olgan holda oâ€˜qitishni tashkil etishdir. Mento AI quyidagicha yordam beradi:
      </p>

      <ul class="list-disc ml-6 text-gray-700 space-y-2">
        <li><strong>Ovozdan matnga</strong> (Voiceâ†’Text) â€” eshitish qiyinligi boâ€˜lganlar uchun dars yozuvi.</li>
        <li><strong>Matndan ovozga</strong> (Textâ†’Voice) â€” koâ€˜rish cheklovlari boâ€˜lganlar uchun darsni ovozli tarzda olish.</li>
        <li><strong>Gesture yordam</strong> â€” qoâ€˜l harakati orqali qatnashishni osonlashtirish (shy learners uchun).</li>
        <li><strong>Real-time dashboard</strong> â€” oâ€˜qituvchilar har bir oâ€˜quvchining ehtiyojini tezda aniqlaydi va moslashadi.</li>
      </ul>

      <p class="mt-4 text-gray-700">
        Natija: maktabda har bir oâ€˜quvchi oâ€˜z ehtiyojiga mos holda ta'lim oladi, undan oâ€˜qituvchilar samaradorlikni oshiradi va erta xatoliklarni bartaraf etadi.
      </p>
    </div>
  </section>

  <!-- CONTACT/FOOTER -->
  <footer class="bg-indigo-900 text-white py-10">
    <div class="max-w-4xl mx-auto px-6 text-center">
      <h3 class="text-xl font-semibold mb-3">Mento AI â€” Demo</h3>
      <p class="text-sm text-indigo-300 mb-4">Murojaat: example@mento.ai</p>
      <p class="mt-6 text-sm text-indigo-300">Â© 2025 Mento AI â€” Demo</p>
    </div>
  </footer>

  <!-- SCRIPTS: STT / TTS / Gesture -->
  <script>
    /* ---------------------------
       Voice â†’ Text (Browser demo)
    ----------------------------*/
    const startBtn = document.getElementById('startStt');
    const stopBtn = document.getElementById('stopStt');
    const sttText = document.getElementById('sttText');

    let recognition;
    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      recognition = new SpeechRecognition();
      recognition.lang = 'uz-UZ'; // yoki 'ru-RU' / 'en-US'
      recognition.interimResults = true;
      recognition.continuous = true;

      recognition.onresult = (event) => {
        let transcript = '';
        for (let i = event.resultIndex; i < event.results.length; ++i) {
          transcript += event.results[i][0].transcript;
        }
        sttText.textContent = transcript;
        // Update live transcript in smart board mock
        const live = document.getElementById('liveTranscript');
        if (live) live.textContent = transcript;
      };

      recognition.onerror = (e) => {
        console.error('STT error', e);
      };
    } else {
      sttText.textContent = 'Brauzeringiz SpeechRecognition ni qoâ€˜llab-quvvatlamaydi.';
      startBtn.disabled = true;
      stopBtn.disabled = true;
    }

    startBtn?.addEventListener('click', () => {
      if (recognition) recognition.start();
    });
    stopBtn?.addEventListener('click', () => {
      if (recognition) recognition.stop();
    });

    /* ---------------------------
       Text â†’ Voice (Browser demo)
    ----------------------------*/
    function speakText() {
      const t = document.getElementById('ttsText').value;
      if (!t) return;
      if ('speechSynthesis' in window) {
        const utter = new SpeechSynthesisUtterance(t);
        // utter.lang = 'uz-UZ'; // brauzer til qoâ€˜llab-quvvatlashiga qarab oâ€˜zgartiring
        window.speechSynthesis.speak(utter);
      } else {
        alert('Brauzeringiz speechSynthesis ni qoâ€˜llab-quvvatlamaydi.');
      }
    }
    function stopSpeak() {
      if ('speechSynthesis' in window) window.speechSynthesis.cancel();
    }

    /* ---------------------------
       Gesture Recognition (MediaPipe Hands demo)
    ----------------------------*/
    const videoElement = document.getElementById('gestureVideo');
    const canvasElement = document.getElementById('gestureCanvas');
    const canvasCtx = canvasElement.getContext('2d');
    const gestureStatus = document.getElementById('gestureStatus');

    // Resize canvas to video size helper
    function resizeCanvasToVideo() {
      canvasElement.width = videoElement.videoWidth || 640;
      canvasElement.height = videoElement.videoHeight || 480;
    }

    if (window.navigator.mediaDevices && window.navigator.mediaDevices.getUserMedia) {
      navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {
        videoElement.srcObject = stream;
        videoElement.onloadeddata = () => {
          resizeCanvasToVideo();
        };
      }).catch((err) => {
        console.error('Camera access denied:', err);
        gestureStatus.textContent = 'Kamera ruxsati kerak';
        gestureStatus.classList.remove('text-green-600');
        gestureStatus.classList.add('text-red-600');
      });
    } else {
      gestureStatus.textContent = 'Kamera mavjud emas yoki brauzer qoâ€˜llab-quvvatlamaydi.';
    }

    // Initialize MediaPipe Hands if available
    if (window.Hands) {
      const hands = new Hands({
        locateFile: (file) => {
          return `https://cdn.jsdelivr.net/npm/@mediapipe/hands/${file}`;
        }
      });

      hands.setOptions({
        maxNumHands: 2,
        modelComplexity: 1,
        minDetectionConfidence: 0.6,
        minTrackingConfidence: 0.5
      });

      hands.onResults((results) => {
        canvasCtx.save();
        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
        canvasCtx.drawImage(results.image, 0, 0, canvasElement.width, canvasElement.height);
        if (results.multiHandLandmarks && results.multiHandLandmarks.length > 0) {
          for (const landmarks of results.multiHandLandmarks) {
            drawConnectors(canvasCtx, landmarks, HAND_CONNECTIONS, { color: '#00FF00', lineWidth: 2 });
            drawLandmarks(canvasCtx, landmarks, { color: '#FF0000', lineWidth: 1 });
          }
          // simple indicator: if any hand detected -> "Qo'l ko'targan"
          gestureStatus.textContent = 'Qo\'l aniqlanmoqda â€” mavjud';
          gestureStatus.classList.remove('text-red-600');
          gestureStatus.classList.add('text-green-600');
        } else {
          gestureStatus.textContent = 'Qo\'l topilmadi';
          gestureStatus.classList.remove('text-green-600');
          gestureStatus.classList.add('text-red-600');
        }
        canvasCtx.restore();
      });

      const camera = new Camera(videoElement, {
        onFrame: async () => {
          await hands.send({ image: videoElement });
        },
        width: 640,
        height: 480
      });
      camera.start();
    } else {
      console.warn('MediaPipe Hands not available in this environment.');
      document.getElementById('gestureStatus').textContent = 'Gesture demo mavjud emas (MediaPipe yuklanmadi).';
    }

    /* ---------------------------
       Production integration notes
    ----------------------------*/
    // 1) Voiceâ†’Text (Whisper): send audio Blob to backend -> call OpenAI Whisper endpoint -> return transcript
    // 2) Textâ†’Voice (ElevenLabs): send text to server -> call ElevenLabs TTS -> stream audio back to client
    // 3) Gesture: run lightweight model on edge (Jetson/RPi) and emit events to server via WebSocket
    // 4) Smart Board: central server (Node.js) aggregates events via WebSocket and pushes to dashboard clients
    // IMPORTANT: store all API keys on server (not in frontend). Use HTTPS and auth for school deployment.

  </script>

</body>
</html>
